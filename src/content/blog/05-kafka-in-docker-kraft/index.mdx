---
title: "Running Kafka Locally with Docker and KRaft Mode"
description: "Learn how to set up and run Apache Kafka locally using Docker with the new KRaft mode, eliminating the need for Zookeeper."
date: "2025-01-27"
draft: true
tags:
  - "kafka"
  - "docker"
  - "kraft"
  - "streaming"
  - "message-queue"
  - "apache-kafka"
  - "kafka-docker"
  - "kafka-kraft-mode"
  - "kafka-without-zookeeper"
  - "local-kafka-setup"
  - "docker-compose-kafka"
  - "kafka-streaming-platform"
  - "kafka-cluster-docker"
  - "kafka-development-environment"
  - "kafka-local-development"
  - "kafka-docker-compose"
  - "kafka-kraft-configuration"
  - "kafka-broker-setup"
  - "kafka-docker-tutorial"
  - "kafka-kraft-cluster"
  - "kafka-docker-configuration"
  - "kafka-local-cluster"
  - "kafka-docker-setup"
  - "kafka-kraft-mode-tutorial"
  - "kafka-with-docker"
  - "kafka-local-development-setup"
  - "kafka-kraft-mode-docker"
  - "kafka-docker-cluster"
  - "kafka-kraft-mode-configuration"
ogImage: "/images/kafka-docker-kraft.png"
---

import { Picture } from "astro:assets";
import OgImage from "./kafka-docker-kraft.png";
import Callout from "@/components/Callout.astro";

---

<Picture
  src={OgImage}
  alt="Running Kafka Locally with Docker and KRaft Mode"
  inferSize
  formats={["avif", "webp"]}
/>

## Introduction

Welcome to this comprehensive guide on setting up Apache Kafka locally using Docker with the innovative KRaft mode. Apache Kafka has revolutionized the way we handle real-time data pipelines and streaming applications. Traditionally, Kafka relied on Zookeeper for cluster coordination, but with the introduction of **KRaft (Kafka Raft Metadata) mode**, Kafka can now operate seamlessly without Zookeeper. This not only simplifies deployment and management but also enhances Kafka's performance and reliability.

Whether you're a developer looking to experiment with Kafka's features, a data engineer setting up a development environment, or someone eager to understand the nuances of Kafka's architecture, this tutorial is tailored for you.

## Prerequisites

Before diving in, ensure you have the following installed on your machine:

- **Docker** (version 20.10.0 or later): Docker provides the necessary environment to run Kafka containers effortlessly.
- **Docker Compose** (version 1.29.0 or later): Docker Compose helps in defining and managing multi-container Docker applications.
- **Basic Understanding of Kafka Concepts**: Familiarity with Kafka's architecture, topics, brokers, and producers/consumers will be beneficial.

> **Pro Tip:** If you're new to Kafka, consider going through some introductory materials to get acquainted with its core concepts.

## Setting Up the Docker Compose File

Let's begin by setting up the `docker-compose.yml` file. This configuration will define a two-node Kafka cluster operating in KRaft mode.

```yaml
version: "3.8"

services:
  kafka1:
    image: bitnami/kafka:3.7
    container_name: kafka1
    ports:
      - "9092:9092"
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka1:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
    volumes:
      - kafka_data:/bitnami/kafka

  kafka2:
    image: bitnami/kafka:3.7
    container_name: kafka2
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=2
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka1:9093,2@kafka2:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
    depends_on:
      - kafka1
    volumes:
      - kafka_data:/bitnami/kafka

volumes:
  kafka_data:
```

### Breakdown of the Configuration

1. **KAFKA_ENABLE_KRAFT=yes**: Activates KRaft mode, enabling Kafka to run without Zookeeper.
2. **KAFKA_CFG_NODE_ID**: Assigns a unique identifier to each Kafka node.
3. **KAFKA_CFG_PROCESS_ROLES**: Defines the roles of the node, in this case, both controller and broker.
4. **KAFKA_CFG_CONTROLLER_QUORUM_VOTERS**: Lists the voters participating in the controller quorum, ensuring high availability.
5. **KAFKA_CFG_LISTENERS**: Specifies the network interfaces Kafka listens on for client and controller communications.
6. **KAFKA_CFG_ADVERTISED_LISTENERS**: Determines the addresses clients will use to connect to Kafka.
7. **Volumes**: Persists Kafka data, ensuring data durability across container restarts.

### Visual Representation

Here's a simple diagram to illustrate the setup:

```markdown
graph LR
kafka1[Kafka Node 1]
kafka2[Kafka Node 2]
kafka1 -->|Controller & Broker| kafka2
kafka2 -->|Controller & Broker| kafka1
```

This diagram showcases the bidirectional communication between the two Kafka nodes, both acting as controllers and brokers.

## Starting the Kafka Cluster

With the configuration in place, let's get the cluster up and running.

1. **Launch the Cluster:**

   Open your terminal, navigate to the directory containing the `docker-compose.yml` file, and execute:

   ```bash
   docker-compose up -d
   ```

   This command starts both Kafka nodes in detached mode.

2. **Verify the Running Containers:**

   To ensure that both Kafka containers are running smoothly, use:

   ```bash
   docker ps
   ```

   You should see both `kafka1` and `kafka2` listed as active containers.

## Creating a Topic

Now that our Kafka cluster is operational, let's create a topic to start producing and consuming messages.

```bash
docker exec -it kafka1 kafka-topics.sh --create \
  --topic test-topic \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 2
```

**Explanation of the Command:**

- **--topic test-topic**: Names the topic as "test-topic".
- **--bootstrap-server localhost:9092**: Connects to the Kafka cluster via the exposed port.
- **--partitions 3**: Divides the topic into 3 partitions for parallelism.
- **--replication-factor 2**: Ensures that each partition is replicated across 2 brokers for fault tolerance.

## Producing and Consuming Messages

Let's test our Kafka setup by sending and receiving messages.

1. **Start a Console Producer:**

   This producer will allow you to send messages to the `test-topic`.

   ```bash
   docker exec -it kafka1 kafka-console-producer.sh \
     --topic test-topic \
     --bootstrap-server localhost:9092
   ```

   Type your messages here and press Enter to send them.

2. **Start a Console Consumer:**

   In a new terminal window, start a consumer to receive messages from the `test-topic`.

   ```bash
   docker exec -it kafka1 kafka-console-consumer.sh \
     --topic test-topic \
     --bootstrap-server localhost:9092 \
     --from-beginning
   ```

   Any messages you send from the producer will appear here in real-time.

## Monitoring and Management

Monitoring is crucial to ensure your Kafka cluster runs smoothly. Here's how you can utilize Kafka's built-in tools:

1. **List All Topics:**

   ```bash
   docker exec -it kafka1 kafka-topics.sh --list --bootstrap-server localhost:9092
   ```

   This command displays all the topics currently available in your Kafka cluster.

2. **Describe a Specific Topic:**

   Get detailed information about `test-topic`:

   ```bash
   docker exec -it kafka1 kafka-topics.sh --describe --topic test-topic --bootstrap-server localhost:9092
   ```

   You'll receive details like the number of partitions, replication factor, and the status of each partition.

## Stopping the Cluster

Once you've completed your tasks, you might want to stop the Kafka cluster to free up resources.

```bash
docker-compose down
```

This command stops and removes the containers and network but retains the Kafka data volume, ensuring your data persists for future sessions.

## Best Practices and Tips

- **Data Persistence:** Always ensure that your Kafka data directories are mounted to Docker volumes to prevent data loss upon container restarts.
- **Scaling:** While this tutorial sets up a two-node cluster, Kafka can scale horizontally by adding more brokers to handle increased load.
- **Security:** For production environments, consider implementing security measures like SSL encryption and proper authentication mechanisms.
- **Monitoring Tools:** Integrate third-party monitoring solutions like Prometheus and Grafana for enhanced visibility into your Kafka cluster's performance.

## Conclusion

Setting up Apache Kafka locally with Docker and KRaft mode is a streamlined approach that eliminates the complexities associated with Zookeeper. This setup is ideal for:

- **Learning Kafka:** Dive deep into Kafka's functionalities without the overhead of managing a full-fledged production environment.
- **Development and Testing:** Develop and test your applications in an environment that mirrors real-world Kafka setups.
- **Prototyping:** Build and experiment with proof-of-concept projects, tweaking configurations as needed.

**Note:** While this setup is excellent for development and testing, it's not recommended for production use. Production environments require considerations for scalability, security, and robust monitoring mechanisms.

<Callout type="info">
  I'm actively seeking opportunities as a Senior Backend Developer. If you're
  looking to enhance your team with experienced talent, feel free to reach out
  to me at <a href="mailto:admin@saybackend.com">admin@saybackend.com</a>.
</Callout>
